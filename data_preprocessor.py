import os
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from tqdm import tqdm
import json
import warnings
import chardet

class Config:
    def __init__(self):
        # æ•°æ®è·¯å¾„
        self.data_paths = [
            # CIC-IDS2017æ•°æ®é›†
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CIC2017\TrafficLabelling_\Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv",
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CIC2017\TrafficLabelling_\Friday-WorkingHours-Morning.pcap_ISCX.csv",
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CIC2017\TrafficLabelling_\Monday-WorkingHours.pcap_ISCX.csv",
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CIC2017\TrafficLabelling_\Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv",
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CIC2017\TrafficLabelling_\Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv",
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CIC2017\TrafficLabelling_\Tuesday-WorkingHours.pcap_ISCX.csv",
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CIC2017\TrafficLabelling_\Wednesday-workingHours.pcap_ISCX.csv",
            # CIC-DDoS2019æ•°æ®é›†
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CICDDoS2019\class_split\BENIGN.csv",
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CICDDoS2019\class_split\DrDoS_DNS.csv",
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CICDDoS2019\class_split\DrDoS_LDAP.csv",
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CICDDoS2019\class_split\DrDoS_MSSQL.csv",
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CICDDoS2019\class_split\DrDoS_NetBIOS.csv",
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CICDDoS2019\class_split\DrDoS_NTP.csv",
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CICDDoS2019\class_split\DrDoS_SNMP.csv",
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CICDDoS2019\class_split\DrDoS_UDP.csv",
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CICDDoS2019\class_split\LDAP.csv",
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CICDDoS2019\class_split\MSSQL.csv",
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CICDDoS2019\class_split\NetBIOS.csv",
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CICDDoS2019\class_split\Portmap.csv",
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CICDDoS2019\class_split\Syn.csv",
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CICDDoS2019\class_split\TFTP.csv",
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CICDDoS2019\class_split\UDP.csv",
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CICDDoS2019\class_split\UDPLag.csv",
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CICDDoS2019\class_split\UDP-lag.csv",
            r"D:\æ¯•ä¸šè®¾è®¡\æ•°æ®é¢„å¤„ç†\raw_data\CICDDoS2019\class_split\WebDDoS.csv"
        ]

        # è¾“å‡ºç›®å½•
        self.output_path = r"D:\bysj\server\passdata"
        
        # éªŒè¯è·¯å¾„
        self.validate_paths()

        # æ”»å‡»ç±»å‹æ ‡ç­¾æ˜ å°„ï¼ˆ9å¤§ç±»ï¼ŒåŸºäºé¦–ä½æ•°å­—ï¼‰
        self.attack_type_mapping = {
            "BENIGN": 0,
            "DoS Hulk": 1,
            "DoS GoldenEye": 1,
            "DoS slowloris": 1,
            "DoS Slowhttptest": 1,
            "DrDoS_NTP": 2,
            "DrDoS_DNS": 2,
            "DrDoS_UDP": 2,
            "DrDoS_MSSQL": 2,
            "DrDoS_NetBIOS": 2,
            "DrDoS_LDAP": 2,
            "DrDoS_SNMP": 2,
            "PortScan": 3,
            "Bot": 4,
            "WebDDoS": 5,
            "Heartbleed": 6,
            "Syn": 7,
            "TFTP": 7,
            "UDP": 7,
            "MSSQL": 7,
            "NetBIOS": 7,
            "LDAP": 7,
            "Portmap": 7,
            "UDPLag": 8,
            "UDP-lag": 8
        }

        # å¤„ç†å‚æ•°
        self.test_size = 0.15
        self.random_state = 42
        self.min_samples_per_class = 10
        self.categorical_cols = ["Protocol"]
        self.variance_threshold = 0.01  # æ–¹å·®é˜ˆå€¼
        self.top_k_features = 50  # åˆå§‹é€‰æ‹©top 50ä¸ªç‰¹å¾ï¼Œå¯æ ¹æ®æ•°æ®è°ƒæ•´

    def validate_paths(self):
        """éªŒè¯æ‰€æœ‰æ•°æ®è·¯å¾„æ˜¯å¦å­˜åœ¨"""
        print("\n[åˆå§‹åŒ–] æ­£åœ¨éªŒè¯æ•°æ®è·¯å¾„...")
        valid_paths = []
        
        for path in self.data_paths:
            if os.path.exists(path):
                valid_paths.append(path)
                print(f"  âœ“ {os.path.basename(path)}")
            else:
                print(f"  âœ— è·¯å¾„ä¸å­˜åœ¨: {path}")
        
        if not valid_paths:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®æ–‡ä»¶ï¼")
        
        self.data_paths = valid_paths
        print(f"éªŒè¯å®Œæˆï¼Œå…±æ‰¾åˆ° {len(valid_paths)} ä¸ªæœ‰æ•ˆæ–‡ä»¶")

class DataPreprocessor:
    def __init__(self, config):
        self.config = config
        warnings.filterwarnings('ignore')

    def detect_encoding(self, file_path):
        """è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç¼–ç """
        with open(file_path, 'rb') as f:
            result = chardet.detect(f.read(100000))
        return result['encoding']

    def load_single_file(self, path):
        """åŠ è½½å•ä¸ªCSVæ–‡ä»¶"""
        try:
            encoding = self.detect_encoding(path)
            try:
                df = pd.read_csv(
                    path, encoding=encoding, engine='python', on_bad_lines='warn',
                    dtype={'Protocol': 'str'}, true_values=['Yes', 'yes', 'TRUE', 'True'],
                    false_values=['No', 'no', 'FALSE', 'False']
                )
            except UnicodeDecodeError:
                df = pd.read_csv(
                    path, encoding='ISO-8859-1', engine='python', on_bad_lines='warn',
                    dtype={'Protocol': 'str'}, true_values=['Yes', 'yes', 'TRUE', 'True'],
                    false_values=['No', 'no', 'FALSE', 'False']
                )
            
            # è‡ªåŠ¨æ£€æµ‹æ ‡ç­¾åˆ—
            label_cols = [col for col in df.columns if 'label' in col.lower()]
            if not label_cols:
                print(f"è­¦å‘Š: æ–‡ä»¶ {os.path.basename(path)} æ²¡æœ‰æ ‡ç­¾åˆ—ï¼Œè·³è¿‡")
                return None
            df = df.rename(columns={label_cols[0]: 'Label'})
            df = df.dropna(subset=['Label'])
            
            if len(df) == 0:
                print(f"è­¦å‘Š: æ–‡ä»¶ {os.path.basename(path)} æ— æœ‰æ•ˆæ•°æ®")
                return None
                
            df['Label'] = df['Label'].str.strip()
            return df
        except Exception as e:
            print(f"åŠ è½½æ–‡ä»¶ {os.path.basename(path)} å¤±è´¥: {str(e)}")
            return None

    def load_data(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®æ–‡ä»¶"""
        print("\n[1/6] æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶...")
        dfs = []
        for path in tqdm(self.config.data_paths, desc="åŠ è½½è¿›åº¦"):
            df = self.load_single_file(path)
            if df is not None:
                dfs.append(df)
                print(f"å·²åŠ è½½: {os.path.basename(path)} (æ ·æœ¬æ•°: {len(df):,})")
        if not dfs:
            raise ValueError("æœªåŠ è½½åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼")
        combined = pd.concat(dfs, ignore_index=True)
        print(f"\næ€»åŠ è½½è®°å½•æ•°: {len(combined):,}")
        print("æ ‡ç­¾ç±»å‹åˆ†å¸ƒ:", combined['Label'].value_counts().to_dict())
        return combined

    def clean_data(self, df):
        """æ•°æ®æ¸…æ´—"""
        print("\n[2/6] æ­£åœ¨æ¸…æ´—æ•°æ®...")
        
        # åˆ é™¤å…¨é›¶åˆ—
        zero_cols = df.columns[(df == 0).all()]
        if zero_cols.any():
            df = df.drop(zero_cols, axis=1)
            print(f"å·²åˆ é™¤å…¨é›¶åˆ—: {list(zero_cols)}")
        
        # å¤„ç†ç¼ºå¤±å€¼
        for col in tqdm(df.columns, desc="å¤„ç†åˆ—"):
            if pd.api.types.is_numeric_dtype(df[col]):
                df[col] = df[col].fillna(df[col].median())
            elif df[col].dtype == 'object':
                df[col] = df[col].fillna('unknown')
        
        # å¤„ç†æ— ç©·å€¼
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        df = df.ffill().bfill()
        
        return df

    def select_discriminative_features(self, df):
        """é€‰æ‹©èƒ½å¤Ÿæ˜æ˜¾ä½“ç°å·®å¼‚çš„ç‰¹å¾"""
        print("\n[3/6] æ­£åœ¨é€‰æ‹©é«˜åŒºåˆ†åº¦ç‰¹å¾...")
        
        # æå–ç‰¹å¾å’Œæ ‡ç­¾
        X = df.drop(columns=['Label'])
        y = df['Label'].map(self.config.attack_type_mapping).fillna(99).astype(int)
        
        # åˆ é™¤å…¨é›¶åˆ—å’Œä½æ–¹å·®åˆ—
        numeric_cols = X.select_dtypes(include=np.number).columns
        variances = X[numeric_cols].var()
        low_variance_cols = variances[variances < self.config.variance_threshold].index
        X = X.drop(columns=low_variance_cols)
        print(f"å·²åˆ é™¤ä½æ–¹å·®åˆ—ï¼ˆæ–¹å·®<{self.config.variance_threshold}ï¼‰: {list(low_variance_cols)}")
        
        # ä½¿ç”¨ANOVA Få€¼é€‰æ‹©topç‰¹å¾
        numeric_cols = X.select_dtypes(include=np.number).columns
        if len(numeric_cols) > 0:
            selector = SelectKBest(score_func=f_classif, k=min(self.config.top_k_features, len(numeric_cols)))
            selector.fit(X[numeric_cols], y)
            scores = pd.Series(selector.scores_, index=numeric_cols)
            selected_cols = scores.sort_values(ascending=False).index[:self.config.top_k_features].tolist()
            print(f"é€‰æ‹©çš„topç‰¹å¾ï¼ˆåŸºäºANOVA Få€¼ï¼‰: {selected_cols}")
        else:
            selected_cols = []
        
        # ç¡®ä¿åŒ…å«Protocolï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'Protocol' in X.columns and 'Protocol' not in selected_cols:
            selected_cols.append('Protocol')
        
        # æ„é€ æœ€ç»ˆæ•°æ®æ¡†
        df_selected = df[selected_cols + ['Label']]
        print(f"æœ€ç»ˆä¿ç•™ç‰¹å¾æ•°: {len(selected_cols)}")
        
        return df_selected

    def process_features(self, df):
        """ç‰¹å¾å·¥ç¨‹"""
        print("\n[4/6] æ­£åœ¨å¤„ç†ç‰¹å¾...")
        
        # åè®®ç¼–ç 
        if 'Protocol' in df.columns:
            le = LabelEncoder()
            df['Protocol'] = le.fit_transform(df['Protocol'].astype(str))
        
        # æ•°å€¼å½’ä¸€åŒ–
        numeric_cols = df.select_dtypes(include=np.number).columns
        numeric_cols = [col for col in numeric_cols if col != 'Label']
        if len(numeric_cols) > 0:
            scaler = MinMaxScaler()
            df[numeric_cols] = scaler.fit_transform(df[numeric_cols])
        
        return df

    def process_labels(self, df):
        """æ ‡ç­¾å¤„ç†ï¼ˆ9å¤§ç±»ï¼‰"""
        print("\n[5/6] æ­£åœ¨å¤„ç†æ ‡ç­¾...")
        print("åŸå§‹æ ‡ç­¾åˆ†å¸ƒ:", df['Label'].value_counts().to_dict())
        
        # åº”ç”¨æ ‡ç­¾æ˜ å°„
        df['Label'] = df['Label'].map(self.config.attack_type_mapping)
        
        # å¤„ç†æœªçŸ¥æ ‡ç­¾
        unknown_labels = df[df['Label'].isna()]['Label'].value_counts()
        if not unknown_labels.empty:
            print("è­¦å‘Š: å‘ç°æœªæ˜ å°„çš„æ ‡ç­¾:", unknown_labels.index.tolist())
        df['Label'] = df['Label'].fillna(99).astype(int)
        
        # è¿‡æ»¤ç¨€æœ‰ç±»åˆ«
        label_counts = df['Label'].value_counts()
        valid_labels = label_counts[label_counts >= self.config.min_samples_per_class].index
        filtered_df = df[df['Label'].isin(valid_labels)]
        
        print("å¤„ç†åæ ‡ç­¾åˆ†å¸ƒ:", filtered_df['Label'].value_counts().to_dict())
        return filtered_df

    def split_and_save(self, df):
        """æ•°æ®åˆ’åˆ†ä¸ä¿å­˜"""
        print("\n[6/6] æ­£åœ¨ä¿å­˜ç»“æœ...")
        train_df, test_df = train_test_split(
            df, test_size=self.config.test_size, stratify=df['Label'],
            random_state=self.config.random_state
        )
        
        os.makedirs(self.config.output_path, exist_ok=True)
        train_path = os.path.join(self.config.output_path, "train_data.csv")
        test_path = os.path.join(self.config.output_path, "test_data.csv")
        train_df.to_csv(train_path, index=False)
        test_df.to_csv(test_path, index=False)
        
        # è®¡ç®—ç‰¹å¾ç»Ÿè®¡
        numeric_cols = train_df.select_dtypes(include=np.number).columns
        numeric_cols = [col for col in numeric_cols if col != 'Label']
        stats = {col: {"max": float(train_df[col].max()), "min": float(train_df[col].min())} for col in numeric_cols}
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            "feature_columns": list(train_df.columns),
            "label_mapping": {k: int(v) for k, v in self.config.attack_type_mapping.items()},
            "train_samples": int(len(train_df)),
            "test_samples": int(len(test_df)),
            "feature_statistics": stats,
            "label_distribution": {
                "train": {int(k): int(v) for k, v in train_df['Label'].value_counts().items()},
                "test": {int(k): int(v) for k, v in test_df['Label'].value_counts().items()}
            }
        }
        
        with open(os.path.join(self.config.output_path, 'metadata.json'), 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print("\n[å®Œæˆ] æ•°æ®å¤„ç†æˆåŠŸï¼")
        print(f"è®­ç»ƒé›†: {train_path} (æ ·æœ¬æ•°: {len(train_df):,})")
        print(f"æµ‹è¯•é›†: {test_path} (æ ·æœ¬æ•°: {len(test_df):,})")
        print("å…ƒæ•°æ®å·²ä¿å­˜è‡³: metadata.json")

    def run_pipeline(self):
        """æ‰§è¡Œå®Œæ•´æµç¨‹"""
        try:
            raw_data = self.load_data()
            cleaned_data = self.clean_data(raw_data)
            selected_data = self.select_discriminative_features(cleaned_data)
            processed_data = self.process_features(selected_data)
            labeled_data = self.process_labels(processed_data)
            self.split_and_save(labeled_data)
            return True
        except Exception as e:
            print(f"\n[é”™è¯¯] å¤„ç†å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

def main():
    print("="*50)
    print("ç½‘ç»œæµé‡æ•°æ®é¢„å¤„ç†ç³»ç»Ÿ - Transformerä¸“ç”¨ç‰ˆ")
    print("="*50)
    try:
        config = Config()
        processor = DataPreprocessor(config)
        if processor.run_pipeline():
            print("\nâœ… å¤„ç†å®Œæˆï¼è¾“å‡ºç›®å½•:", config.output_path)
        else:
            print("\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯")
    except Exception as e:
        print(f"\nğŸ”¥ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    main()